{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:11:50.617721Z",
     "start_time": "2020-10-28T15:11:50.604757Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:46:35.565864Z",
     "iopub.status.busy": "2020-10-28T14:46:35.564866Z",
     "iopub.status.idle": "2020-10-28T14:46:36.525088Z",
     "shell.execute_reply": "2020-10-28T14:46:36.525088Z",
     "shell.execute_reply.started": "2020-10-28T14:46:35.565864Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import statistics as st\n",
    "import random     as rnd\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import inflection\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy                 import stats  as ss\n",
    "from boruta                import BorutaPy\n",
    "from matplotlib            import pyplot as plt\n",
    "from IPython.display       import Image\n",
    "from IPython.core.display  import HTML\n",
    "\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:11:52.206009Z",
     "start_time": "2020-10-28T15:11:52.177022Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:47:04.711707Z",
     "iopub.status.busy": "2020-10-28T14:47:04.711707Z",
     "iopub.status.idle": "2020-10-28T14:47:04.739632Z",
     "shell.execute_reply": "2020-10-28T14:47:04.738690Z",
     "shell.execute_reply.started": "2020-10-28T14:47:04.711707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cross_validation( x_training, kfold, model_name, model, verbose = False ):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        if verbose:\n",
    "            print( '\\nKFold Number: {}'.format( k ) )\n",
    "            \n",
    "        # start and end date for validation \n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "        # training\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 ) \n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(  m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {'Model Name': model_name,\n",
    "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n",
    "\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "     \n",
    "    \n",
    "def mean_absolute_percentage_error( y, yhat ):\n",
    "    return np.mean( np.abs( ( y - yhat ) / y ) )\n",
    "\n",
    "    \n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    \n",
    "    return pd.DataFrame( { 'Model Name': model_name, \n",
    "                           'MAE': mae, \n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] )\n",
    "\n",
    "def cramer_v( x, y ):\n",
    "    cm = pd.crosstab( x, y ).values\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "\n",
    "\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    plt.rcParams['font.size'] = 22\n",
    "    \n",
    "    #display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:11:53.046858Z",
     "start_time": "2020-10-28T15:11:53.033819Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:47:05.535733Z",
     "iopub.status.busy": "2020-10-28T14:47:05.534733Z",
     "iopub.status.idle": "2020-10-28T14:47:05.559661Z",
     "shell.execute_reply": "2020-10-28T14:47:05.558632Z",
     "shell.execute_reply.started": "2020-10-28T14:47:05.535733Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 0.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:11:54.963728Z",
     "start_time": "2020-10-28T15:11:54.474446Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:47:06.858838Z",
     "iopub.status.busy": "2020-10-28T14:47:06.857839Z",
     "iopub.status.idle": "2020-10-28T14:47:07.396155Z",
     "shell.execute_reply": "2020-10-28T14:47:07.396155Z",
     "shell.execute_reply.started": "2020-10-28T14:47:06.858838Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "df_sales_raw = pd.read_csv('train.csv', sep=',', low_memory=False)\n",
    "df_store_raw = pd.read_csv('store.csv', sep=',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:11:55.625667Z",
     "start_time": "2020-10-28T15:11:55.259399Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:47:07.442032Z",
     "iopub.status.busy": "2020-10-28T14:47:07.442032Z",
     "iopub.status.idle": "2020-10-28T14:47:07.815531Z",
     "shell.execute_reply": "2020-10-28T14:47:07.814562Z",
     "shell.execute_reply.started": "2020-10-28T14:47:07.442032Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge and View\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')\n",
    "df_raw.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1.0 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Understanding Variables\n",
    "**Id** - an Id that represents a (Store, Date) duple within the test set\n",
    "\n",
    "**Store** - a unique Id for each store\n",
    "\n",
    "**Sales** - the turnover for any given day (this is what you are predicting)\n",
    "\n",
    "**Customers** - the number of customers on a given day\n",
    "\n",
    "**Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "\n",
    "**State Holiday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public \n",
    "holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "\n",
    "**School Holiday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "\n",
    "**Store Type** - differentiates between 4 different store models: a, b, c, d\n",
    "\n",
    "**Assortment** - describes an assortment level: a = basic, b = extra, c = extended\n",
    "\n",
    "**Competition Distance** - distance in meters to the nearest competitor store\n",
    "\n",
    "**Competition Open Since Month / Year** - gives the approximate year and month of the time the nearest competitor was opened\n",
    "\n",
    "**Promo** - indicates whether a store is running a promo on that day\n",
    "\n",
    "**Promo 2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
    "\n",
    "**Promo 2 Since Year / Week** - describes the year and calendar week when the store started participating in Promo2\n",
    "\n",
    "**Promo Interval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round \n",
    "starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.1 Rename Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:25:48.598070Z",
     "iopub.status.busy": "2020-10-28T13:25:48.597072Z",
     "iopub.status.idle": "2020-10-28T13:25:48.647936Z",
     "shell.execute_reply": "2020-10-28T13:25:48.647936Z",
     "shell.execute_reply.started": "2020-10-28T13:25:48.598070Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the data to avoid future problems in the original data\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original Columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform into Snake Case\n",
    "cols_old = [ 'Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', \n",
    "            'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance','CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval' ]\n",
    "\n",
    "snakecase = lambda x: inflection.underscore (x)\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "# Rename\n",
    "df1.columns = cols_new\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.2 Data Dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of Rows and Columns\n",
    "print('Number of Rows:', df1.shape[0])\n",
    "print('Number of Columns:', df1.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Variable Types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Date column to date format\n",
    "df1 ['date'] = pd.to_datetime(df1['date'])\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.4 NA´s Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check all NA´s\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.5 Fill out NA´s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming NA´s in competition_distance are stores too far away (200.000,00) - Business Question\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "# Assuming NA´s in competition_open_since_month can be the same as the date of the date column (future review CRISP-DS)    \n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis = 1)\n",
    "\n",
    "# Assuming NA´s in competition_open_since_year can be the same as the date of the date column and higher than 2000 (future review CRISP-DS)\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis = 1)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].apply(lambda x: 2000.0 if x < 2000.0 else x)\n",
    "\n",
    "# Assuming NA´s in promo2_since_week can be the same as the date of the date column (future review CRISP-DS)\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis = 1)\n",
    "\n",
    "# Assuming NA´s in promo2_since_year can be the same as the date of the date column (future review CRISP-DS)                \n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis = 1)\n",
    "\n",
    "# Assuming NA´s in promo_interval can be compared to a list and then substitute them for 0 or 1\n",
    "# List to compare\n",
    "month_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec' }\n",
    "\n",
    "# Fill NA´s with 0\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "# Creating a new column month_map maping with date column and the list month_map (numbers will be subtitute for strings)                         \n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "                                    \n",
    "# Creating a new column is_promo comparing promo_interval with month_map and returnig 0 or 1    \n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample\n",
    "df1.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.6 Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Competition and promo2 as interger\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype ( int ) \n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype ( int ) \n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype ( int )\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype ( int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1.7 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Numerical and Categorical Attributes\n",
    "num_var = df1.select_dtypes( include = ['int64', 'float64'])\n",
    "cat_var = df1.select_dtypes( exclude = ['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Central Tendency - mean, meadian \n",
    "ct1 = pd.DataFrame( num_var.apply( np.mean) ).T\n",
    "ct2 = pd.DataFrame( num_var.apply( np.median) ).T\n",
    "ct3 = pd.DataFrame( num_var.apply( st.mode ) ).T \n",
    "\n",
    "# Dispersion\n",
    "d1 = pd.DataFrame( num_var.apply( np.std) ).T\n",
    "d2 = pd.DataFrame( num_var.apply( min) ).T\n",
    "d3 = pd.DataFrame( num_var.apply( max) ).T\n",
    "d4 = pd.DataFrame( num_var.apply( lambda x: x.max() - x.min() ) ).T\n",
    "d5 = pd.DataFrame( num_var.apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( num_var.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# Concatenate\n",
    "ds = pd.concat([ct1, ct2, ct3, d1, d2, d3, d4, d5, d6 ]).T.reset_index()\n",
    "ds.columns = ['attributes', 'mean', 'median', 'mode', 'std', 'min', 'max', 'range', 'skew', 'kurtosis']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.7.2 Categorial Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Values of categorical\n",
    "cat_var.apply( lambda x: x.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Boxplot's of state holidays, store types and assortment types by sales\n",
    "aux = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.boxplot( x='state_holiday', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.boxplot( x='store_type', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.boxplot( x='assortment', y='sales', data=aux )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 2.1 Hyphotesis Mental Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image( 'img/MindMapHypothesis.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 2.2 Hypothesis List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**1.** Stores with higher assortments should sell more.\n",
    "\n",
    "**2.** Stores with closer competitors should sell less \n",
    "\n",
    "**3.** Stores with longer competitors should sell more.\n",
    "\n",
    "**4.** Stores with active promotions should sell more.\n",
    "\n",
    "**5.** Stores with more consecutive promotions should sell more.\n",
    "\n",
    "**6.** Stores open during the Christmas holiday should sell more.\n",
    "\n",
    "**7.** Stores should sell more over the years.\n",
    "\n",
    "**8.** Stores should sell more in the second half of the year.\n",
    "\n",
    "**9.** Stores should sell less on weekends.\n",
    "\n",
    "**10.** Stores should sell less during school holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 2.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# year\n",
    "df2[ 'year' ] = df2[ 'date' ].dt.year\n",
    "\n",
    "# month\n",
    "df2[ 'month' ] = df2[ 'date' ].dt.month\n",
    "\n",
    "# day\n",
    "df2[ 'day' ] = df2[ 'date' ].dt.day\n",
    "\n",
    "# week of year\n",
    "df2[ 'week_of_year' ] = df2[ 'date' ].dt.weekofyear\n",
    "\n",
    "# year week\n",
    "df2[ 'year_week' ] = df2[ 'date' ].dt.strftime( '%Y-%W' )\n",
    "\n",
    "# competition since\n",
    "df2[ 'competition_since' ] = df2.apply( lambda x: datetime.datetime( year=x[ 'competition_open_since_year' ], month=x[ 'competition_open_since_month' ],day=1 ), axis=1 )\n",
    "\n",
    "df2[ 'competition_time_month' ] = ( ( df2[ 'date' ] - df2[ 'competition_since' ] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# promo since\n",
    "df2[ 'promo_since' ] = df2[ 'promo2_since_year' ].astype( str ) + '-' + df2[ 'promo2_since_week' ].astype( str )\n",
    "\n",
    "df2[ 'promo_since' ] = df2[ 'promo_since' ].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "\n",
    "df2[ 'promo_time_week' ] = ( ( df2[ 'date' ] - df2[ 'promo_since' ] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# assortment\n",
    "df2[ 'assortment' ] = df2[ 'assortment' ].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "# state holiday\n",
    "df2[ 'state_holiday' ] = df2[ 'state_holiday' ].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3.0 Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 3.1 Line Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The stores should be open and sales should be higher than 0\n",
    "df3 = df3[ ( df3[ 'open' ] != 0 ) & ( df3[ 'sales' ] >0 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 3.2 Column Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_drop = [ 'customers', 'open', 'promo_interval', 'month_map' ]\n",
    "df3 = df3.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4.0 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.1 Resposnse Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Checking sales normality (response variable)\n",
    "df4 = df3.copy()\n",
    "sns.distplot( df4 [ 'sales' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.2 Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Update Numerical and Categorical Variables\n",
    "num_var = df4.select_dtypes( include = ['int64', 'float64'])\n",
    "cat_var = df4.select_dtypes( exclude = ['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_var.hist( bins = 50 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.1.3 Categoriacal Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "plt.subplot( 3, 2, 1 )\n",
    "a = df4[ df4[ 'state_holiday' ] != 'regular_day' ]\n",
    "sns.countplot( a[ 'state_holiday' ] )\n",
    "\n",
    "plt.subplot( 3, 2, 2 )\n",
    "sns.kdeplot( df4[ df4[ 'state_holiday' ] == 'public_holiday' ] [ 'sales' ], label = 'public_holiday', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'state_holiday' ] == 'easter_holiday' ] [ 'sales' ], label = 'easter_holiday', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'state_holiday' ] == 'christmas' ] [ 'sales' ], label = 'christmas', shade = True )\n",
    "\n",
    "# store_type\n",
    "plt.subplot( 3, 2, 3 )\n",
    "sns.countplot( df4[ 'store_type' ] )\n",
    "\n",
    "plt.subplot( 3, 2, 4 )\n",
    "sns.kdeplot( df4[ df4[ 'store_type' ] == 'a' ] [ 'sales' ], label = 'a', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'store_type' ] == 'b' ] [ 'sales' ], label = 'b', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'store_type' ] == 'c' ] [ 'sales' ], label = 'c', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'store_type' ] == 'd' ] [ 'sales' ], label = 'd', shade = True )\n",
    "\n",
    "# assortment\n",
    "plt.subplot( 3, 2, 5 )\n",
    "sns.countplot( df4[ 'assortment' ] )\n",
    "\n",
    "plt.subplot( 3, 2, 6 )\n",
    "sns.kdeplot( df4[ df4[ 'assortment' ] == 'extended' ] [ 'sales' ], label = 'extended', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'assortment' ] == 'basic' ] [ 'sales' ], label = 'basic', shade = True )\n",
    "sns.kdeplot( df4[ df4[ 'assortment' ] == 'extra' ] [ 'sales' ], label = 'extra', shade = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.2 Bivariate Analysis - Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H1 Stores with higher assortments should sell more.\n",
    "#### False - Stores with less assortment sell more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['assortment', 'sales']].groupby( 'assortment' ).sum().reset_index()\n",
    "sns.barplot( x = 'assortment', y ='sales', data = aux1 );\n",
    "plt.xlabel('');\n",
    "plt.ylabel('Sales', size = 10);\n",
    "plt.xticks(size=10)\n",
    "plt.yticks(size=10)\n",
    "plt.title('Assortment By Sales', size = 15);\n",
    "\n",
    "aux2 = df4[['month', 'assortment', 'sales']].groupby( ['month','assortment'] ).sum().reset_index()\n",
    "aux2.pivot( index='month', columns='assortment', values='sales' ).plot()\n",
    "plt.legend(prop={\"size\":10})\n",
    "plt.xticks(size=10)\n",
    "plt.yticks(size=10)\n",
    "plt.xlabel(\"\");\n",
    "plt.title('Assortment By Sales over Months', size = 15);\n",
    "\n",
    "aux3 = df4[['year_week', 'assortment', 'sales']].groupby( ['year_week','assortment'] ).sum().reset_index()\n",
    "aux3.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n",
    "plt.legend(prop={\"size\":10})\n",
    "plt.xticks(size=10)\n",
    "plt.yticks(size=10)\n",
    "plt.xlabel(\"\");\n",
    "plt.title('Assortment By Sales over Weeks of the Year', size = 15);\n",
    "\n",
    "aux4 = aux3[aux3['assortment'] == 'extra']\n",
    "aux4.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n",
    "plt.legend(prop={\"size\":10})\n",
    "plt.xticks(size=10)\n",
    "plt.yticks(size=10)\n",
    "plt.xlabel(\"\");\n",
    "plt.title('Extra Assortment By Sales over Weeks of the Year', size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H2 Stores with closer competitors should sell less\n",
    "#### False - Stores with close competitors sell more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 12))\n",
    "aux1 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
    "bins = list( np.arange( 0, 20000, 1000) )\n",
    "aux1['competition_distance_binned'] = pd.cut( aux1[ 'competition_distance' ], bins = bins )\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby( 'competition_distance_binned' ).sum().reset_index()\n",
    "sns.barplot( x='competition_distance_binned', y = 'sales', data = aux2, palette = 'YlGnBu_r' );\n",
    "plt.xticks( rotation = 70, size = 10 )\n",
    "plt.yticks(size = 10)\n",
    "plt.xlabel(\"Competition Distance\", size = 15)\n",
    "plt.ylabel(\"Sales\", size = 15)\n",
    "plt.title('Competition Distance By Sales', size = 15);\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.subplot( 1, 2, 1 )\n",
    "sns.scatterplot( x ='competition_distance', y='sales', data = aux1);\n",
    "plt.xticks(size=10);\n",
    "plt.yticks(size=10);\n",
    "plt.xlabel(\"Competition Distance\", size = 15);\n",
    "plt.ylabel(\"Sales\", size = 15);\n",
    "plt.title('Competition Distance By Sales', size = 15);\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );\n",
    "plt.xticks(size=10);\n",
    "plt.yticks(size=10);\n",
    "plt.title('Competition Distance By Sales', size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H3 Stores with long term competitors should sell more\n",
    "\n",
    "#### False - Stores with short term competitors sell more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12), clear=True)\n",
    "plt.subplot(2, 2, 1)\n",
    "aux1 = df4[['competition_open_since_year', 'sales']].groupby('competition_open_since_year').sum().reset_index()\n",
    "sns.barplot( x = 'competition_open_since_year', y = 'sales', data = aux1, palette = 'YlGnBu');\n",
    "plt.xticks(size=10, rotation=70);\n",
    "plt.yticks(size=10);\n",
    "plt.xlabel(\"Competition Opened Since Year\");\n",
    "plt.ylabel(\"Sales\");\n",
    "plt.title('Competition Opened Since Year By Sales', size = 15);\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(data=aux1, x=\"competition_open_since_year\", y=\"sales\")\n",
    "plt.title('Competition Opened Since Year By Sales', size = 15);\n",
    "plt.xticks(size=10);\n",
    "plt.yticks(size=10);\n",
    "plt.xlabel('Years', size = 10);\n",
    "plt.ylabel('Sales', size = 10);\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(aux1.corr(method=\"pearson\"), annot = True);\n",
    "plt.xticks(size=10);\n",
    "plt.yticks(size=10);\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.regplot( x='competition_open_since_year', y='sales', data=aux1 );\n",
    "plt.xticks(size=10);\n",
    "plt.yticks(size=10);\n",
    "plt.xlabel('Years', size = 10);\n",
    "plt.ylabel('Sales', size = 10);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H4 Stores with active promotions should sell more\n",
    "#### True - Stores in promo 1 sell more\n",
    "#### False - Stores in extended promo (promo 2) sell less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['promo', 'promo2', 'sales']].groupby(['promo', 'promo2']).sum().reset_index()\n",
    "aux1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hypotesis Test\n",
    "model = ols('sales ~ promo * promo2', data = df4).fit()\n",
    "result = sm.stats.anova_lm(model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.subplot ( 1, 2, 1)\n",
    "sns.barplot( data = aux1, x = 'promo', y = 'sales')\n",
    "\n",
    "plt.subplot ( 1, 2, 2)\n",
    "sns.barplot( data = aux1, x = 'promo2', y = 'sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H5 Stores with more consecutive promotions sell more\n",
    "#### False - Stores with more consecutive promotions seel less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[['promo', 'promo2', 'sales']].groupby( ['promo', 'promo2'] ).sum().reset_index()\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "aux2.plot( ax=ax )\n",
    "\n",
    "ax.legend( labels=['Tradicional & Extendida', 'Extendida']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H6 Stores open during the Christmas holiday should sell more.\n",
    "#### False - Stores open during the Christmas holiday should sell less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "\n",
    "plt.subplot( 1, 2, 1 )\n",
    "aux1 = aux[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\n",
    "sns.barplot( x='state_holiday', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "aux2 = aux[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\n",
    "sns.barplot( x='year', y='sales', hue='state_holiday', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H7 Stores should sell more over the years.\n",
    "#### False - Stores sell less over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='year', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='year', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H8 Stores should sell more in the second half of the year.\n",
    "#### False - Stores sell less in the second half of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='month', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='month', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H9 Stores should sell less on weekends.\n",
    "#### True - Stores sell less on weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='day_of_week', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='day_of_week', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### H10 Stores should sell less during school holidays.\n",
    "#### True - Stores sell less during school holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\n",
    "plt.subplot( 2, 1, 1 )\n",
    "sns.barplot( x='school_holiday', y='sales', data=aux1 );\n",
    "\n",
    "aux2 = df4[['month', 'school_holiday', 'sales']].groupby( ['month','school_holiday'] ).sum().reset_index()\n",
    "plt.subplot( 2, 1, 2 )\n",
    "sns.barplot( x='month', y='sales', hue='school_holiday', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.2.1 Hypotesis Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### H1 - False -> Conclusion - Stores with less assortment sell more\n",
    "\n",
    "#### H2 - False -> Conclusion - Stores with close competitors sell more\n",
    "\n",
    "#### H3 - False -> Conclusion - Stores with short term competitors sell more\n",
    "\n",
    "#### H4 - False / True -> Conclusion - Stores in promo 1 sell more and stores in extended promo (promo 2) sell less\n",
    "\n",
    "#### H5 - False -> Conclusion - Stores with more consecutive promotions seel less\n",
    "\n",
    "#### H6 - False -> Conclusion - Stores open during the Christmas holiday sell less\n",
    "\n",
    "#### H7 - False -> Conclusion - Stores sell less over the years\n",
    "\n",
    "#### H8 - False -> Conclusion - Stores sell less in the second half of the year\n",
    "\n",
    "#### H9 - True -> Conclusion - Stores sell less on weekends\n",
    "\n",
    "#### H10 - True -> Conclusion - Stores sell less during school holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.3.1 Numerical Attribites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# All correlations\n",
    "correl = num_var.corr( method = 'pearson' )\n",
    "sns.heatmap( correl, annot = True );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Only categorical variables\n",
    "a = df4.select_dtypes( include = 'object' )\n",
    "a.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate Cramer V\n",
    "a1 = cramer_v( a['state_holiday'], a['state_holiday'] )\n",
    "a2 = cramer_v( a['state_holiday'], a['store_type'] )\n",
    "a3 = cramer_v( a['state_holiday'], a['assortment'] )\n",
    "\n",
    "a4 = cramer_v( a['store_type'], a['state_holiday'] )\n",
    "a5 = cramer_v( a['store_type'], a['store_type'] )\n",
    "a6 = cramer_v( a['store_type'], a['assortment'] )\n",
    "\n",
    "a7 = cramer_v( a['assortment'], a['state_holiday'] )\n",
    "a8 = cramer_v( a['assortment'], a['store_type'] )\n",
    "a9 = cramer_v( a['assortment'], a['assortment'] )\n",
    "\n",
    "# Final dataset\n",
    "d = pd.DataFrame( {'state_holiday': [a1, a2, a3], \n",
    "               'store_type': [a4, a5, a6],\n",
    "               'assortment': [a7, a8, a9]  })\n",
    "d = d.set_index( d.columns )\n",
    "\n",
    "sns.heatmap( d, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5.0 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 5.1 Normalization\n",
    "### No normalization required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 5.2 Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# competition distance\n",
    "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump( rs, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_competition_distance.pkl', 'wb' ) )\n",
    "\n",
    "# competition time month\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump( rs, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_competition_time_month.pkl', 'wb' ) )\n",
    "\n",
    "# promo time week\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump( rs, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_promo_time_week.pkl', 'wb' ) )\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump( rs, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_year.pkl', 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 5.3 Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5.3.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
    "pickle.dump( le, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\encoding_store_type.pkl', 'wb' ) )\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5.3.2 Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p( df5['sales'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.distplot( df5 ['sales'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5.3.3 Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day \n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6.0 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 6.1 Split dataframe into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ] \n",
    "df6 = df6.drop( cols_drop, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6[['store', 'date' ]].groupby( 'store' ).max().reset_index()['date'][0] - datetime.timedelta( days = 6*7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# Test Dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 6.2 Boruta as Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Train and Test for Boruta\n",
    "#X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "#y_train_n = y_train.values.ravel()\n",
    "#\n",
    "## Random Forest Regressor\n",
    "#rf = RandomForestRegressor( n_jobs=-1 )\n",
    "#\n",
    "## Define Boruta\n",
    "#boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42).fit(X_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 6.2.1 Best Boruta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "#cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# Best Features\n",
    "#X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# Not Seceted Boruta\n",
    "#cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cols_selected_boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 6.3 Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 7.0 Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No inclsuion of sales and date yet\n",
    "x_train = X_train[ cols_selected_boruta ]\n",
    "x_test = X_test[ cols_selected_boruta ]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = X_train[ cols_selected_boruta_full ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.1. Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_test.copy()\n",
    "\n",
    "# Prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby( 'store' ).mean().reset_index().rename( columns={'sales': 'predictions'} )\n",
    "aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
    "\n",
    "# Estimation Baseline\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "# Performance functions to calculate the perfomance to all future models\n",
    "# We have transformed 'sales' with the log function, so we need to undo using exponencial function\n",
    "baseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.2 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "lr = LinearRegression().fit( x_train, y_train )\n",
    "\n",
    "# Prediction\n",
    "yhat_lr = lr.predict( x_test )\n",
    "\n",
    "# Performance\n",
    "lr_result = ml_error( \"Linear Regression\", np.expm1( y_test ), np.expm1 (yhat_lr) ) \n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.2.1 Linear Regression Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation( x_training, 5, \"Linear Regression\", lr, verbose = False )\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 7.3 Linear Regression Regularized - LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "lrr = Lasso( alpha = 0.01).fit( x_train, y_train )\n",
    "\n",
    "# Prediction\n",
    "yhat_lrr = lr.predict( x_test )\n",
    "\n",
    "# Performance\n",
    "lrr_result = ml_error( \"Linear Regression - Lasso\", np.expm1( y_test ), np.expm1( yhat_lrr ) )\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.3.1 Lasso Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation( x_training, 5, 'Lasso', lrr, verbose = False )\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 7.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "rf = RandomForestRegressor( n_estimators = 100, n_jobs = -1, random_state = 42 ).fit( x_train, y_train )\n",
    "\n",
    "# Prediction\n",
    "yhat_rf = rf.predict ( x_test )\n",
    "\n",
    "# Performance\n",
    "rf_result = ml_error( \"Random Forest Regressor\", np.expm1( y_test ), np.expm1( yhat_rf ) )\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.4.1 Random Forest Regressor Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor', rf, verbose = True )\n",
    "rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 7.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model_xgb = xgb.XGBRegressor( objective = 'reg:squarederror',\n",
    "                              n_estimators = 100,\n",
    "                              eta = 0.01,\n",
    "                              max_depth = 10,\n",
    "                              subsample = 0.7,\n",
    "                              colsample_bytree=0.9 ).fit( x_train, y_train ).fit( x_train, y_train )\n",
    "\n",
    "# Prediction\n",
    "yhat_xgb = model_xgb.predict( x_test )\n",
    "\n",
    "# Performance\n",
    "xgb_result = ml_error( 'XBGoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.5.1 XGBoost Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation( x_training, 5, \"XGBoost Regressor\", model_xgb, verbose = True )\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 7.6 Model's Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.6.1 Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result = pd.concat( [ baseline_result, lr_result, lrr_result, rf_result, xgb_result ] )\n",
    "modelling_result.sort_values( 'RMSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 7.6.2 Real Performance - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat( [ lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv ] )\n",
    "modelling_result_cv.sort_values( 'RMSE CV' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 8.0 Hyperparamenter Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 8.1 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#param = {\n",
    "#    'n_estimators': [1500, 1700, 2500, 3000, 3500],\n",
    "#    'eta': [0.01, 0.03],\n",
    "#    'max_depth': [3, 5, 9],\n",
    "#    'subsample': [0.1, 0.5, 0.7],\n",
    "#    'colsample_bytree': [0.3, 0.7, 0.9],\n",
    "#    'min_child_weight': [3, 8, 15]\n",
    "#        }\n",
    "#\n",
    "#MAX_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#final_result = pd.DataFrame()\n",
    "#\n",
    "#for i in range( MAX_EVAL ):\n",
    "#    # Choose parameters values randomly\n",
    "#    hp = { k: rnd.sample( v, 1 )[0] for k, v in param.items() }\n",
    "#    print( hp )\n",
    "#    \n",
    "#    # Model\n",
    "#    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "#                                  n_estimators=hp['n_estimators'], \n",
    "#                                  eta=hp['eta'], \n",
    "#                                  max_depth=hp['max_depth'], \n",
    "#                                  subsample=hp['subsample'],\n",
    "#                                  colsample_bytree=hp['colsample_bytree'],\n",
    "#                                  min_child_weight=hp['min_child_weight'] )\n",
    "#\n",
    "#    # Performance\n",
    "#    result = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose = True )\n",
    "#    final_result = pd.concat( [final_result, result] )\n",
    "#        \n",
    "#final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 8.2 Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_tuned = {\n",
    "    'n_estimators': 3500,\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'min_child_weight': 8 \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=param_tuned['n_estimators'], \n",
    "                                    eta=param_tuned['eta'], \n",
    "                                    max_depth=param_tuned['max_depth'], \n",
    "                                    subsample=param_tuned['subsample'],\n",
    "                                    min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
    "\n",
    "# Prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# Performance\n",
    "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "pickle.dump( model_xgb_tuned, open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\model\\model_rossmann.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "mpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "MPE shows overestimated predictions - stocked itens can be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 9.0 Error Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df9 = X_test[ cols_selected_boruta_full ]\n",
    "\n",
    "# Reescaling\n",
    "df9[ 'sales' ] = np.expm1( df9[ 'sales' ] )\n",
    "df9[ 'predictions' ] = np.expm1( yhat_xgb_tuned )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 9.1 Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sum of Predictions\n",
    "df91 = df9[['store', 'predictions']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "# MAE an MAPE\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns = {0:'MAE'} ) \n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns = {0:'MAPE'} ) \n",
    "\n",
    "# Merge\n",
    "df9_aux3 = pd.merge( df9_aux1, df9_aux2, how = \"inner\", on = \"store\" )\n",
    "df92 = pd.merge( df91, df9_aux3, how = 'inner', on = 'store' )\n",
    "\n",
    "# Scenarios\n",
    "df92[ 'worst_scenario' ] = df92[ 'predictions' ] - df92[ 'MAE' ]\n",
    "df92[ 'best_scenario' ] = df92[ 'predictions' ] + df92[ 'MAE' ]\n",
    "\n",
    "# Order Columns\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df92.sort_values( 'MAPE', ascending=False ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot( x='store', y='MAPE', data=df92 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 9.2 Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df93 = df92[[ 'predictions', 'worst_scenario', 'best_scenario' ]].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\n",
    "df93[ 'Values' ] = df93[ 'Values' ].map( 'R${:,.2f}'.format )\n",
    "df93                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 9.3 Machien Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.subplot( 2, 2, 1 )\n",
    "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
    "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' ).set_title( 'Sales x Predictions')\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.lineplot( x='date', y='error_rate', data=df9 ).set_title( 'Error Rate' )\n",
    "plt.axhline( 1, linestyle='--')\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.distplot( df9['error'] ).set_title( 'Error Distribution')\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.scatterplot( df9['predictions'], df9['error'] ).set_title( 'Residual Analysis' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:27:30.926023Z",
     "iopub.status.busy": "2020-10-28T13:27:30.926023Z",
     "iopub.status.idle": "2020-10-28T13:27:30.951940Z",
     "shell.execute_reply": "2020-10-28T13:27:30.950982Z",
     "shell.execute_reply.started": "2020-10-28T13:27:30.926023Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Rossmann(object):\n",
    "    def __init__( self ):\n",
    "        self.rescaling_competition_distance      = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_competition_distance.pkl', 'rb' ) )\n",
    "        self.rescaling_competition_time_month    = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_competition_time_month.pkl', 'rb' ) )\n",
    "        self.rescaling_promo_time_week           = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_promo_time_week.pkl', 'rb' ) )\n",
    "        self.rescaling_year                      = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\rescaling_year.pkl', 'rb' ) )\n",
    "        self.encoding_store_type                 = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\parameters\\encoding_store_type.pkl', 'rb' ) )\n",
    "\n",
    "        \n",
    "    def data_cleaning( self, df1 ):\n",
    "\n",
    "        # Rename columns into Snake Case\n",
    "        cols_old = [ 'Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', \n",
    "                     'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance','CompetitionOpenSinceMonth',\n",
    "                     'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval' ]\n",
    "    \n",
    "        snakecase = lambda x: inflection.underscore (x)\n",
    "        cols_new = list( map( snakecase, cols_old ) )\n",
    "    \n",
    "        # Rename\n",
    "        df1.columns = cols_new\n",
    "    \n",
    "        # Date column to date format\n",
    "        df1 ['date'] = pd.to_datetime(df1['date'])\n",
    "        df1.dtypes\n",
    "     \n",
    "        # Fill out NA´s\n",
    "        # Assuming NA´s in competition_distance are stores too far away (200.000,00) - Business Question\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "        \n",
    "        # Assuming NA´s in competition_open_since_month can be the same as the date of the date column (future review CRISP-DS)    \n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis = 1)\n",
    "        \n",
    "        # Assuming NA´s in competition_open_since_year can be the same as the date of the date column and higher than 2000 (future review CRISP-DS)\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis = 1)\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].apply(lambda x: 2000.0 if x < 2000.0 else x)\n",
    "        \n",
    "        # Assuming NA´s in promo2_since_week can be the same as the date of the date column (future review CRISP-DS)\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis = 1)\n",
    "        \n",
    "        # Assuming NA´s in promo2_since_year can be the same as the date of the date column (future review CRISP-DS)                \n",
    "        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis = 1)\n",
    "        \n",
    "        # Assuming NA´s in promo_interval can be compared to a list and then substitute them for 0 or 1\n",
    "        # List to compare\n",
    "        month_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec' }\n",
    "        \n",
    "        # Fill NA´s with 0\n",
    "        df1['promo_interval'].fillna(0, inplace=True)\n",
    "        \n",
    "        # Creating a new column month_map maping with date column and the list month_map (numbers will be subtitute for strings)                         \n",
    "        df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "                                            \n",
    "        # Creating a new column is_promo comparing promo_interval with month_map and returnig 0 or 1    \n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)                                     \n",
    "        \n",
    "     \n",
    "        # Change Types\n",
    "        # Competition and promo2 as interger\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype ( int ) \n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype ( int ) \n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype ( int )\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype ( int )\n",
    "        \n",
    "        return df1\n",
    "\n",
    "    def feature_engineering( self, df2 ):\n",
    "        \n",
    "        # year\n",
    "        df2[ 'year' ] = df2[ 'date' ].dt.year\n",
    "\n",
    "        # month\n",
    "        df2[ 'month' ] = df2[ 'date' ].dt.month\n",
    "\n",
    "        # day\n",
    "        df2[ 'day' ] = df2[ 'date' ].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2[ 'week_of_year' ] = df2[ 'date' ].dt.weekofyear\n",
    "\n",
    "        # year week\n",
    "        df2[ 'year_week' ] = df2[ 'date' ].dt.strftime( '%Y-%W' )\n",
    "\n",
    "        # competition since\n",
    "        df2[ 'competition_since' ] = df2.apply( lambda x: datetime.datetime( year=x[ 'competition_open_since_year' ], month=x[ 'competition_open_since_month' ],day=1 ), axis=1 )\n",
    "\n",
    "        df2[ 'competition_time_month' ] = ( ( df2[ 'date' ] - df2[ 'competition_since' ] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # promo since\n",
    "        df2[ 'promo_since' ] = df2[ 'promo2_since_year' ].astype( str ) + '-' + df2[ 'promo2_since_week' ].astype( str )\n",
    "\n",
    "        df2[ 'promo_since' ] = df2[ 'promo_since' ].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "\n",
    "        df2[ 'promo_time_week' ] = ( ( df2[ 'date' ] - df2[ 'promo_since' ] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # assortment\n",
    "        df2[ 'assortment' ] = df2[ 'assortment' ].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "        # state holiday\n",
    "        df2[ 'state_holiday' ] = df2[ 'state_holiday' ].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n",
    "        # Variable Selection\n",
    "        # Line Filtering\n",
    "        # The stores should be open and sales should be higher than 0\n",
    "        df2 = df2[ df2[ 'open' ] != 0 ]\n",
    "\n",
    "        # Column Filtering\n",
    "        cols_drop = [ 'open', 'promo_interval', 'month_map' ]\n",
    "        df2 = df2.drop( cols_drop, axis=1 )\n",
    "        \n",
    "        return df2\n",
    "\n",
    "    \n",
    "    def data_preparation ( self, df5 ):\n",
    "\n",
    "        # Rescaling\n",
    "        # competition distance\n",
    "        df5['competition_distance'] = self.rescaling_competition_distance.fit_transform( df5[['competition_distance']].values )\n",
    "\n",
    "        # competition time month\n",
    "        df5['competition_time_month'] = self.competition_time_month.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        # promo time week\n",
    "        df5['promo_time_week'] = self.promo_time_week.fit_transform( df5[['promo_time_week']].values )\n",
    "\n",
    "        # year\n",
    "        df5['year'] = self.year.fit_transform( df5[['year']].values )\n",
    "\n",
    "        # Transform\n",
    "        # Encoding\n",
    "        # state_holiday - One Hot Encoding\n",
    "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "        # store_type - Label Encoding\n",
    "        df5['store_type'] = self.enconding_store_type.fit_transform( df5['store_type'] )\n",
    "\n",
    "        # assortment - Ordinal Encoding\n",
    "        assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "        # Nature Transformation\n",
    "        # day of week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "        # day \n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "        \n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "                          'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo_time_week',\n",
    "                          'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "        \n",
    "        return df5[ cols_selected ]\n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        \n",
    "        # Prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # Join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "        \n",
    "        return original_data.to_json( orient = 'records', date_format = 'iso' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 10.2 API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T15:12:44.112459Z",
     "start_time": "2020-10-28T15:12:43.845359Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask             import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\model\\model_rossmann.pkl', 'rb') )\n",
    "\n",
    "# Initialize API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "   \n",
    "    if test_json: # there is data\n",
    "        if instance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # Data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        \n",
    "        # Feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        \n",
    "        # Data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        \n",
    "        # Prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return Reponse( '{}', status=200, mimetype='application/json' )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '127.0.0.1' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:31:54.144442Z",
     "start_time": "2020-01-12T20:31:54.030073Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T13:26:49.100126Z",
     "iopub.status.busy": "2020-10-28T13:26:49.099128Z",
     "iopub.status.idle": "2020-10-28T13:26:49.133042Z",
     "shell.execute_reply": "2020-10-28T13:26:49.133042Z",
     "shell.execute_reply.started": "2020-10-28T13:26:49.100126Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv( r'C:\\Users\\arros\\OneDrive\\ciencia_de_dados\\data_science_em_producao\\Rossmann-Kaggle\\test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:32:49.000535Z",
     "start_time": "2020-01-12T20:32:48.869756Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T13:26:49.975913Z",
     "iopub.status.busy": "2020-10-28T13:26:49.974926Z",
     "iopub.status.idle": "2020-10-28T13:26:50.012784Z",
     "shell.execute_reply": "2020-10-28T13:26:50.012784Z",
     "shell.execute_reply.started": "2020-10-28T13:26:49.975913Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [20, 23, 22] )]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:32:49.118371Z",
     "start_time": "2020-01-12T20:32:49.083025Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T14:46:45.509395Z",
     "iopub.status.busy": "2020-10-28T14:46:45.509395Z",
     "iopub.status.idle": "2020-10-28T14:46:45.684266Z",
     "shell.execute_reply": "2020-10-28T14:46:45.683263Z",
     "shell.execute_reply.started": "2020-10-28T14:46:45.509395Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert Dataframe to json\n",
    "import json\n",
    "import requests\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:32:51.321365Z",
     "start_time": "2020-01-12T20:32:49.295302Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-28T13:26:51.034289Z",
     "iopub.status.busy": "2020-10-28T13:26:51.033255Z",
     "iopub.status.idle": "2020-10-28T13:26:51.051208Z",
     "shell.execute_reply": "2020-10-28T13:26:51.050279Z",
     "shell.execute_reply.started": "2020-10-28T13:26:51.034289Z"
    }
   },
   "outputs": [],
   "source": [
    "# API Call\n",
    "url = 'http://127.0.0.1:5000/rossmann/predict'\n",
    "#url = 'https://rossmann-model-test.herokuapp.com/rossmann/predict'\n",
    "header = {'Content-type': 'application/json' } \n",
    "data = data\n",
    "\n",
    "r = requests.post( url, data = data, headers = header )\n",
    "print( 'Status Code {}'.format( r.status_code ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:26:51.482440Z",
     "iopub.status.busy": "2020-10-28T13:26:51.482440Z",
     "iopub.status.idle": "2020-10-28T13:26:51.491416Z",
     "shell.execute_reply": "2020-10-28T13:26:51.490419Z",
     "shell.execute_reply.started": "2020-10-28T13:26:51.482440Z"
    }
   },
   "outputs": [],
   "source": [
    "print( r.json )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df1.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:32:51.364571Z",
     "start_time": "2020-01-12T20:32:51.325667Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T20:32:51.406254Z",
     "start_time": "2020-01-12T20:32:51.380244Z"
    }
   },
   "outputs": [],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format( \n",
    "            d2.loc[i, 'store'], \n",
    "            d2.loc[i, 'prediction'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
